<h2>The Problem</h2>
<p>At my company, we use <a href="https://airflow.apache.org/">Apache Airflow</a> to orchestrate (almost) all of our BI processes. Its flexibility to define complex
dependencies between tasks, the power of modeling it by code and the huge open source community are some of the reasons
why we think Airflow is a great tool for the job.</p>

<p>We had the need to trigger Airflow DAGs that would process data uploaded to S3 without defined time frames. When that
data is renewed, we want an immediate move from S3 to our Data Warehouse (Redshift). Since we do not know when the data
will be renewed, we can not define the DAGs to be executed at any certain time.</p>

<p>There is an implementation already on Airflow that covers this use case: the <a href="https://www.astronomer.io/guides/what-is-a-sensor">S3 Sensor</a>.
Which periodically pulls information from S3 to check if a new file has been uploaded. However, we still need to run a DAG periodically that
triggers the sensor task. If the files are not changed frequently (2-3 months), it seems a waste to have a DAG run every
second, minute or hour to execute the sensor. Moreover, if you increase the delay within executions, then the response
time between the change on S3 and the final result in the Data Warehouse might not be acceptable.</p>

<h2>The Solution</h2>
<p>In our case, we wanted that, once a file is uploaded in S3, a DAG is immediately triggered. For the above mentioned
reasons, we do not want the DAG scheduled every second. Therefore, we developed a simple system that involves S3 events,
a Lambda function and a DynamoDB to handle all the flows that must be triggered right after the data has arrived.</p>

{% load static %}
{% with "blog/images/entries/S3TriggerAirflowDAG.png" as image %}
<img class="img_responsive" src="{% static image %}" />
{% endwith %}

<p>The image is pretty self-explanatory. We configure an S3 Event for a specific prefix and suffix, so when a file is
uploaded that matches the expression, it triggers a Lambda function and passes the S3 Key to it. The lambda checks the
DynamoDB to know which Airflow DAG to run for that key. We use the <a href="https://airflow.apache.org/docs/apache-airflow/stable/rest-api-ref.html">experimental REST API</a>
to trigger the DAG (starting Airflow 2.0, there is a <a href="https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html">stable REST API</a>).</p>

<p>The only other detail worth mentioning is what we store in the DynamoDB:</p>
<ul>
    <li>Trigger name to have context on it</li>
    <li>DAG id that will be run</li>
    <li>Bucket where the file is expected</li>
    <li>Prefix of the file</li>
    <li>Suffix of the file</li>
    <li>Boolean to activate or deactivate triggers</li>
</ul>

<p>Both prefix and suffix are used to have more flexibility on the input files we get. Especially for cases where each new
file is stored in a different timestamp subfolder.</p>

<h2>The Conclusion</h2>
<p>There could be counter arguments for having this extra piece of architecture for this use case: lambda + dynamo extra
costs, extra architecture complexity or that it adds another point of failure in the system. There could even be other
alternatives that we are not aware of. However, at the moment in time that we came up with this solution and our use
case (a few csv files updated by external processes randomly every 2 to 6 months), it looked good and simple enough for
us.</p>
