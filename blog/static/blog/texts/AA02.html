<p>Ensuring data quality is a very important part of any Data Platform, especially for mature ones with multiple data sources, complex business data transformations and users. Delivered data must be <strong>complete</strong>, <strong>accurate</strong> and <strong>timely</strong>. Otherwise, business users can not rely on it to make informed decisions. They must be able to <strong>trust</strong> the data.</p>
<p>Once the organization has made the shift towards Data Driven Decision Making (not an easy task!), we have the responsibility to maintain the “machine” running properly on production. The same way a web page, a software application or a factory need to run without fail, data delivery must run without fail.</p>
<p>However, we have the handicap that we can be “easily” tossed aside. If a web application is continuously down, the business has to throw any resource needed at it to fix it. If a dashboard has continuously incorrect data, the business user will get tired of it, will go straight to the source, and will create some nasty unmaintainable ad-hoc excel for reporting.</p>

<h2 id="common-causes-of-data-quality-issues">Common causes of data quality issues</h2>
<ul>
    <li><em>Code changes</em>: A new version of a transformation process introduces a bug, or a change in one table with multiple dependencies does not take into account all of them.</li>
    <li><em>Bad source data</em>: Incorrect data is generated by the source. Incorrect numbers, duplicated data, etc.</li>
    <li><em>Missing data or parts of it</em>: The extractor process stops working, someone moves a csv file to another location and no one notices, etc.</li>
    <li><em>Timeliness</em>: Data arriving late. Some users might check the dashboard first thing in the morning every day! (Crazy right?)</li>
</ul>

<br>
<h2 id="reactive-vs-proactive-problem-solving">Reactive vs Proactive problem solving</h2>
<br>
<h3 id="reactive">Reactive</h3>
<p>We are reactive when we fix problems when something breaks in Production. A user informing that a dashboard does not look good or is broken, a machine learning model that stops working trying to read the data, etc. This is obviously bad and should be reduced to the minimum.</p>
<p>However, it is almost impossible to prevent all errors from happening. It is important to be ready and have a flexible flow to fix the issues fast. Some ideas to address this scenario could be:</p>
<ul>
    <li>Have good communication with end users and encourage them to report any issues</li>
    <li>Have good logging on all processes to identify the root of the errors faster</li>
    <li>Have a good Continuous Deploy system to push fixes faster</li>
    <li>Have good idempotent processes to independently re-run the affected parts.</li>
</ul>

<h3 id="proactive">Proactive</h3>
<p>Being proactive when it comes to data quality issues does not only mean to prevent errors from happening, but also be aware of the problems early. The main point is to find the errors before the end user does. Then we can fix it, or at least let the user know that the inaccuracy exists so they do not feel frustrated when they find the dashboard broken.</p>
<p>Some ideas to prevent or identify errors:</p>
<ul>
    <li><em>Unit tests</em>: Try to test all individual code transformation functions or logical groups of them.</li>
    <li><em>Integration tests</em>: Test the entire process against sample data. Trying to incorporate as many corner cases as possible.</li>
    <li><em>Asserts inside the transformation processes</em>: Adding checks inside the (E)T(L) that break the job, trigger alarms or write warnings on the logs specifying the data issues.</li>
    <li><em>Simplify/Centralize all transformations in as few places as possible</em>. For example, not having a transformation job + a view on the database + more mapping directly on the reporting tool. For complex and regularly changing business logic it can be tedious and difficult, but nevertheless it is important to have it in mind.</li>
    <li><em>Constantly query production data with checks</em>:
        <ul>
            <li>NULLs, missing primary/foreign keys, etc.</li>
            <li>Cross check source data vs final data if possible</li>
            <li>Have known queries with expected results</li>
            <li>Have anomaly detection processes</li>
        </ul>
    </li>
</ul>
<p>On top of that, we must ensure that the written code follows good practises as in any other software system.</p>
<p>There are multiple third-party companies that provide tools to address data quality issues. However, since I have no personal experience with any of them, I do not feel informed enough to mention any.</p>

<br>
<h2 id="how-far-should-we-go-">How far should we go?</h2>
<p>Well, although it is very important to take data quality seriously, as data engineers with limited resources, it is impossible to have perfect platforms with perfect data. We must balance reactive and proactive problem solving.</p>
<p>The typical graph that explains the quality vs effort of any project:</p>

{% load static %}
{% with "blog/images/entries/DataQualityVsEffort.png" as image %}
<img class="img_responsive" src="{% static image %}" />
{% endwith %}

<br>
<br>
<h2 id="conclusions">Conclusions</h2>
<p>Any mature Data Platform must have some sort of processes to ensure the quality of the served data. We have to prevent as many issues as possible, keeping in mind that it is impossible to prevent all possible scenarios, therefore being flexible to apply hot-fixes. Finally, as in any kind of software development, striving for excellence on the quality side, but balancing it with new requirements.</p>
